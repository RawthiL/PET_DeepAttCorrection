{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time as time\n",
    "\n",
    "import pydicom as dicom\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import ndimage\n",
    "from skimage import morphology\n",
    "\n",
    "\n",
    "sys.path.insert(0, '../')\n",
    "from DeepAttCorr_lib import data_handling as DH\n",
    "\n",
    "import tensorflow as tf\n",
    "print('Using TensorFlow version: '+tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to dicom files\n",
    "PathDicom = 'DICOM/HNSCC'\n",
    "# Dataset (tfrecord) output path\n",
    "OUTPUT_PATH = './'\n",
    "\n",
    "# Dataset Full-Resolution size\n",
    "Objective_size = np.array([256,256,512])\n",
    "# Number of down-samples dataset to be created.\n",
    "# The smallest dataset will have a size of Objective_size/(2**(DOWNSAMPLE_SCALES-1))\n",
    "DOWNSAMPLE_SCALES = 8\n",
    "\n",
    "# Fraction of dataset used for validation\n",
    "FRAC_VALIDATION = 0.10\n",
    "# Random selection of validation samples (Hard-Coded for reproducibility)\n",
    "# Leave empty to randomly pick the samples.\n",
    "validation_patients_ID = ['HNSCC-01-0024', \n",
    "                          'HNSCC-01-0097', \n",
    "                          'HNSCC-01X-0169', \n",
    "                          'HNSCC-01-0032', \n",
    "                          'HNSCC-01-0062', \n",
    "                          'HNSCC-01-0148', \n",
    "                          'HNSCC-01-0137']\n",
    "\n",
    "\n",
    "# Set different thresholds for the segmentation of the CT images\n",
    "# Thresholds are in Hounsfield Units\n",
    "HU_Air_limit = -1000\n",
    "HU_Lung_upper_limit = -500\n",
    "HU_Fluids_Fat_lower_limit = -125\n",
    "HU_Fluids_Fat_upper_limit = 10.0\n",
    "HU_Parenchyma_upper_limit = 90.0\n",
    "HU_Parenchyma_lower_limit = 10.0\n",
    "HU_Bone_low_limit = 90.0\n",
    "HU_Bone_upper_limit = 1300\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta, datasetName = os.path.split(PathDicom)\n",
    "\n",
    "sample_old = ''\n",
    "name_old = ''\n",
    "current_type = ''\n",
    "\n",
    "no_attenuation_corrected_names = ['PETNAC', 'no ac', 'no_ac', 'not ac', 'not_ac']\n",
    "CT_names = ['ct atten cor', 'CT Soft', 'CT Atten', 'CT Body']\n",
    "\n",
    "# Create dataframe\n",
    "col_names =  ['PatientID', 'Study', 'Type', 'Slices', 'Axial_size','Location']\n",
    "HNSCC_dataframe = pd.DataFrame(columns=col_names) \n",
    "\n",
    "study_count = -1\n",
    "for dirName, subdirList, fileList in os.walk(PathDicom):\n",
    "\n",
    "    # If it is the base directory, we get the samples names\n",
    "    ruta, aux_name = os.path.split(dirName)\n",
    "    if aux_name == datasetName:\n",
    "        sample_name_list = subdirList\n",
    "        print('Found %d samples'%len(sample_name_list))\n",
    "\n",
    "    if len(subdirList) != 0:\n",
    "        continue\n",
    "\n",
    "    # Get name of current sample\n",
    "    base_aux, name_sample = os.path.split(dirName)\n",
    "    base_aux, name_study = os.path.split(base_aux)\n",
    "    if 'simulation' in name_study.lower():\n",
    "        continue\n",
    "    base_aux, sample_act = os.path.split(base_aux)\n",
    "\n",
    "    if sample_act != sample_old:\n",
    "        print('Scanning sample: %s'%sample_act)\n",
    "        sample_old = sample_act\n",
    "\n",
    "\n",
    "    if name_study != name_old:\n",
    "        print('\\tStudy: %s'%name_study)\n",
    "        name_old = name_study\n",
    "\n",
    "\n",
    "    if any(nac_name.lower() in name_sample.lower() for nac_name in no_attenuation_corrected_names):\n",
    "        sys.stdout.write('\\t\\tNAC PET sample:')\n",
    "        current_type = 'PET_NO_AC'\n",
    "    elif any(ct_name.lower() in name_sample.lower() for ct_name in CT_names):\n",
    "        sys.stdout.write('\\t\\tCorr CT sample:')\n",
    "        current_type = 'CT'\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    axial_size = DH.get_sample_axial_FOV_limits(dirName)\n",
    "\n",
    "    # Print sample and sample size\n",
    "    print('\\t\\t\\t%s -- %d'%(name_sample, len(fileList)))\n",
    "    print('\\t\\t\\t\\t\\t\\tAxial_FoV -- [%0.2f ; %0.2f]'%(axial_size[0], axial_size[1]))\n",
    "\n",
    "\n",
    "    # Add to dataframe\n",
    "    HNSCC_dataframe.loc[len(HNSCC_dataframe)] = [sample_act, name_study, current_type, len(fileList), axial_size, dirName]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean dataset from non-matched samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_list = HNSCC_dataframe.PatientID.unique()\n",
    "\n",
    "# Hand removed studies\n",
    "study_ban_list = ['06-03-2002-PETCT HEAD  NECK CA-86406']\n",
    "\n",
    "\n",
    "for patient in patient_list:\n",
    "\n",
    "    # get patient dataframe\n",
    "    patient_df = HNSCC_dataframe.loc[HNSCC_dataframe['PatientID'] == patient]\n",
    "\n",
    "    # get all studies of this patient\n",
    "    study_list = patient_df.Study.unique()\n",
    "\n",
    "    for study_id in study_list:\n",
    "\n",
    "        kill_data_point = False\n",
    "\n",
    "        # get study dataframe\n",
    "        study_df = patient_df.loc[patient_df['Study'] == study_id]\n",
    "\n",
    "        # If the number of unique image types is not at least 2, erase.\n",
    "        if len(study_df.Type.unique()) < 2:\n",
    "            kill_data_point = True\n",
    "\n",
    "        elif any(ban_name.lower() in study_id.lower() for ban_name in study_ban_list):\n",
    "            kill_data_point = True\n",
    "\n",
    "        else:          \n",
    "            # Check the number of study types\n",
    "            pet_df = study_df.loc[study_df['Type'] == 'PET_NO_AC']\n",
    "            ct_df = study_df.loc[study_df['Type'] == 'CT']\n",
    "\n",
    "            num_ct = len(ct_df.index)\n",
    "            num_pet = len(pet_df.index)\n",
    "\n",
    "            # Get the axial FoV limits of each image\n",
    "            ct_axial_limits = np.zeros((num_ct,2))\n",
    "            for idx_ct in range(num_ct):\n",
    "                ct_axial_limits[idx_ct,:] = ct_df.Axial_size.values[idx_ct]\n",
    "            pet_axial_limits = np.zeros((num_pet,2))\n",
    "            for idx_pet in range(num_pet):\n",
    "                pet_axial_limits[idx_pet,:] = pet_df.Axial_size.values[idx_pet]\n",
    "\n",
    "\n",
    "            # get the FoV likelihood\n",
    "            simil = np.zeros((num_ct,num_pet))\n",
    "            for idx_ct in range(num_ct):\n",
    "                for idx_pet in range(num_pet):\n",
    "                    simil[idx_ct,idx_pet] = (ct_axial_limits[idx_ct,0]-pet_axial_limits[idx_pet,0])**2 + (ct_axial_limits[idx_ct,1]-pet_axial_limits[idx_pet,1])**2\n",
    "\n",
    "            # Get the most similar studies\n",
    "            ct_min, pet_min = np.unravel_index(np.argmin(simil), simil.shape)\n",
    "\n",
    "            # Get the minimal overlap limit\n",
    "            lim_inf = np.max([ct_axial_limits[ct_min,0],pet_axial_limits[pet_min,0]])\n",
    "            lim_sup = np.min([ct_axial_limits[ct_min,1],pet_axial_limits[pet_min,1]])\n",
    "\n",
    "            # If they are not conected delete entry...\n",
    "            if lim_inf>lim_sup:\n",
    "                kill_data_point = True\n",
    "            else:\n",
    "                # Kill other studies datapoints \n",
    "                HNSCC_dataframe = HNSCC_dataframe.drop(HNSCC_dataframe.index[(HNSCC_dataframe.PatientID == patient) & (HNSCC_dataframe.Study == study_id) & (HNSCC_dataframe.Type == 'CT') & (HNSCC_dataframe.Location != ct_df.Location.values[ct_min]) ])\n",
    "                HNSCC_dataframe = HNSCC_dataframe.drop(HNSCC_dataframe.index[(HNSCC_dataframe.PatientID == patient) & (HNSCC_dataframe.Study == study_id) & (HNSCC_dataframe.Type == 'PET_NO_AC') & (HNSCC_dataframe.Location != pet_df.Location.values[pet_min]) ])\n",
    "\n",
    "\n",
    "        if kill_data_point:\n",
    "            HNSCC_dataframe = HNSCC_dataframe.drop(HNSCC_dataframe.index[(HNSCC_dataframe.PatientID == patient) & (HNSCC_dataframe.Study == study_id) ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in samples\n",
    "\n",
    "During this process we complete a new database with the volume information before converting and normalizing volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "col_names =  ['PatientID',\n",
    "              'Study',\n",
    "              'CT_Vol_Shape_X', \n",
    "              'CT_Vol_Shape_Y', \n",
    "              'CT_Vol_Shape_Z', \n",
    "              'CT_Voxel_size_X', \n",
    "              'CT_Voxel_size_Y', \n",
    "              'CT_Voxel_size_Z', \n",
    "              'CT_FoV_Size_X', \n",
    "              'CT_FoV_Size_Y', \n",
    "              'CT_FoV_Size_Z', \n",
    "              'CT_FoV_X_min',\n",
    "              'CT_FoV_X_max',\n",
    "              'CT_FoV_Y_min',\n",
    "              'CT_FoV_Y_max',\n",
    "              'CT_FoV_Z_min',\n",
    "              'CT_FoV_Z_max',\n",
    "              'PET_Vol_Shape_X', \n",
    "              'PET_Vol_Shape_Y',\n",
    "              'PET_Vol_Shape_Z',\n",
    "              'PET_Voxel_size_X', \n",
    "              'PET_Voxel_size_Y',\n",
    "              'PET_Voxel_size_Z',\n",
    "              'PET_FoV_Size_X',\n",
    "              'PET_FoV_Size_Y',\n",
    "              'PET_FoV_Size_Z',\n",
    "              'PET_FoV_X_min',\n",
    "              'PET_FoV_X_max',\n",
    "              'PET_FoV_Y_min',\n",
    "              'PET_FoV_Y_max',\n",
    "              'PET_FoV_Z_min',\n",
    "              'PET_FoV_Z_max']\n",
    "\n",
    "\n",
    "DICOM_dataframe = pd.DataFrame(columns=col_names)\n",
    "\n",
    "\n",
    "# For each patient\n",
    "patient_list = HNSCC_dataframe.PatientID.unique()\n",
    "idx_pat = 0\n",
    "for patient in patient_list:\n",
    "\n",
    "    idx_pat = idx_pat + 1\n",
    "    print('(%d/%d) Loading patient: %s'%(idx_pat,len(patient_list), patient))\n",
    "\n",
    "\n",
    "    # and each study\n",
    "    patient_df = HNSCC_dataframe.loc[HNSCC_dataframe['PatientID'] == patient]\n",
    "    # get all studies of this patient\n",
    "    study_list = patient_df.Study.unique()\n",
    "\n",
    "    for study_id in study_list:\n",
    "        # And create one entry...\n",
    "\n",
    "        ct_path = HNSCC_dataframe.Location.loc[HNSCC_dataframe.index[(HNSCC_dataframe.PatientID == patient) & (HNSCC_dataframe.Study == study_id) & (HNSCC_dataframe.Type == 'CT')  ]].values[0]\n",
    "        pet_path = HNSCC_dataframe.Location.loc[HNSCC_dataframe.index[(HNSCC_dataframe.PatientID == patient) & (HNSCC_dataframe.Study == study_id) & (HNSCC_dataframe.Type == 'PET_NO_AC')  ]].values[0]\n",
    "\n",
    "        # Get all slice paths\n",
    "        list_paths_ct = sorted(os.listdir(ct_path))\n",
    "        list_paths_pet = sorted(os.listdir(pet_path))\n",
    "\n",
    "\n",
    "        # Read first slice and allocate memory\n",
    "        RefDs = dicom.read_file(os.path.join(ct_path,list_paths_ct[0]))\n",
    "        ct_cols = RefDs.Columns\n",
    "        ct_rows = RefDs.Rows\n",
    "        ct_slices = len(list_paths_ct)\n",
    "        ct_x_size = RefDs.PixelSpacing[0]\n",
    "        ct_y_size = RefDs.PixelSpacing[1]\n",
    "        ct_z_size = RefDs.SliceThickness\n",
    "        ct_FOV_X = ct_cols*ct_x_size\n",
    "        ct_FOV_Y = ct_rows*ct_y_size\n",
    "        ct_FOV_Z = ct_slices*ct_z_size\n",
    "        ct_FOV_X_min = -ct_FOV_X/2.0\n",
    "        ct_FOV_X_max = ct_FOV_X/2.0\n",
    "        ct_FOV_Y_min = -ct_FOV_Y/2.0\n",
    "        ct_FOV_Y_max = ct_FOV_Y/2.0\n",
    "#         ct_FOV_Z_min = -ct_FOV_Z/2.0\n",
    "#         ct_FOV_Z_max = ct_FOV_Z/2.0\n",
    "        axial_aux = HNSCC_dataframe.Axial_size.loc[HNSCC_dataframe.index[(HNSCC_dataframe.PatientID == patient) & (HNSCC_dataframe.Study == study_id) & (HNSCC_dataframe.Type == 'CT')  ]].values[0]\n",
    "        ct_FOV_Z_min = axial_aux[0]\n",
    "        ct_FOV_Z_max = axial_aux[1]\n",
    "\n",
    "\n",
    "\n",
    "        RefDs = dicom.read_file(os.path.join(pet_path,list_paths_pet[0]))\n",
    "        pet_cols = RefDs.Columns\n",
    "        pet_rows = RefDs.Rows\n",
    "        pet_slices = len(list_paths_pet)\n",
    "        pet_x_size = RefDs.PixelSpacing[0]\n",
    "        pet_y_size = RefDs.PixelSpacing[1]\n",
    "        pet_z_size = ct_z_size\n",
    "        pet_FOV_X = pet_cols*pet_x_size\n",
    "        pet_FOV_Y = pet_rows*pet_y_size\n",
    "        pet_FOV_Z = pet_slices*pet_z_size\n",
    "        pet_FOV_X_min = -pet_FOV_X/2.0\n",
    "        pet_FOV_X_max = pet_FOV_X/2.0\n",
    "        pet_FOV_Y_min = -pet_FOV_Y/2.0\n",
    "        pet_FOV_Y_max = pet_FOV_Y/2.0\n",
    "#         pet_FOV_Z_min = -pet_FOV_Z/2.0\n",
    "#         pet_FOV_Z_max = pet_FOV_Z/2.0\n",
    "        axial_aux = HNSCC_dataframe.Axial_size.loc[HNSCC_dataframe.index[(HNSCC_dataframe.PatientID == patient) & (HNSCC_dataframe.Study == study_id) & (HNSCC_dataframe.Type == 'PET_NO_AC')  ]].values[0]\n",
    "        pet_FOV_Z_min = axial_aux[0]\n",
    "        pet_FOV_Z_max = axial_aux[1]\n",
    "\n",
    "        # Add to dataframe\n",
    "        DICOM_dataframe.loc[len(DICOM_dataframe)] = [patient, \n",
    "                                                     study_id,\n",
    "                                                     ct_cols,\n",
    "                                                     ct_rows, \n",
    "                                                     ct_slices,\n",
    "                                                     ct_x_size, \n",
    "                                                     ct_y_size, \n",
    "                                                     ct_z_size,\n",
    "                                                     ct_FOV_X,\n",
    "                                                     ct_FOV_Y,\n",
    "                                                     ct_FOV_Z, \n",
    "                                                     ct_FOV_X_min,\n",
    "                                                     ct_FOV_X_max,\n",
    "                                                     ct_FOV_Y_min,\n",
    "                                                     ct_FOV_Y_max,\n",
    "                                                     ct_FOV_Z_min,\n",
    "                                                     ct_FOV_Z_max,\n",
    "                                                     pet_cols, \n",
    "                                                     pet_rows, \n",
    "                                                     pet_slices, \n",
    "                                                     pet_x_size, \n",
    "                                                     pet_y_size, \n",
    "                                                     pet_z_size,\n",
    "                                                     pet_FOV_X,\n",
    "                                                     pet_FOV_Y,\n",
    "                                                     pet_FOV_Z,\n",
    "                                                     pet_FOV_X_min,\n",
    "                                                     pet_FOV_X_max,\n",
    "                                                     pet_FOV_Y_min,\n",
    "                                                     pet_FOV_Y_max,\n",
    "                                                     pet_FOV_Z_min,\n",
    "                                                     pet_FOV_Z_max]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('CT FoV Limits:\\n \\t(%0.2f , %0.2f)\\n \\t(%0.2f , %0.2f)\\n \\t(%0.2f , %0.2f)'%(DICOM_dataframe.CT_FoV_X_min.min(),DICOM_dataframe.CT_FoV_X_max.max(),\n",
    "                                                                        DICOM_dataframe.CT_FoV_Y_min.min(),DICOM_dataframe.CT_FoV_Y_max.max(),\n",
    "                                                                        DICOM_dataframe.CT_FoV_Z_min.min(),DICOM_dataframe.CT_FoV_Z_max.max()))\n",
    "\n",
    "print('PET FoV Limits:\\n \\t(%0.2f , %0.2f)\\n \\t(%0.2f , %0.2f)\\n \\t(%0.2f , %0.2f)'%(DICOM_dataframe.PET_FoV_X_min.min(),DICOM_dataframe.PET_FoV_X_max.max(),\n",
    "                                                                        DICOM_dataframe.PET_FoV_Y_min.min(),DICOM_dataframe.PET_FoV_Y_max.max(),\n",
    "                                                                        DICOM_dataframe.PET_FoV_Z_min.min(),DICOM_dataframe.PET_FoV_Z_max.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get absolute FoV limits\n",
    "Lim_X_min = min([DICOM_dataframe.CT_FoV_X_min.min(),DICOM_dataframe.PET_FoV_X_min.min()])\n",
    "Lim_Y_min = min([DICOM_dataframe.CT_FoV_Y_min.min(),DICOM_dataframe.PET_FoV_Y_min.min()])\n",
    "Lim_Z_min = min([DICOM_dataframe.CT_FoV_Z_min.min(),DICOM_dataframe.PET_FoV_Z_min.min()])\n",
    "\n",
    "Lim_X_max = max([DICOM_dataframe.CT_FoV_X_max.max(),DICOM_dataframe.PET_FoV_X_max.max()])\n",
    "Lim_Y_max = max([DICOM_dataframe.CT_FoV_Y_max.max(),DICOM_dataframe.PET_FoV_Y_max.max()])\n",
    "Lim_Z_max = max([DICOM_dataframe.CT_FoV_Z_max.max(),DICOM_dataframe.PET_FoV_Z_max.max()])\n",
    "\n",
    "# Calculate voxel size\n",
    "Voxel_size = [(Lim_X_max-Lim_X_min)/float(Objective_size[0]),\n",
    "              (Lim_Y_max-Lim_Y_min)/float(Objective_size[1]),\n",
    "              (Lim_Z_max-Lim_Z_min)/float(Objective_size[2])]\n",
    "\n",
    "# Transform dataset\n",
    "\n",
    "\n",
    "\n",
    "print('Volume size:')\n",
    "print(Objective_size)\n",
    "print('Voxel size:')\n",
    "print(Voxel_size)\n",
    "\n",
    "low_res_post_scaling = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_patients_list = DICOM_dataframe.PatientID.unique()\n",
    "num_uniques = len(unique_patients_list)\n",
    "print(\"%d unique patients.\"%num_uniques)\n",
    "\n",
    "num_validation = np.ceil(num_uniques*FRAC_VALIDATION)\n",
    "\n",
    "# If no validation patient list was selected, they are randomly chosen\n",
    "if len(validation_patients_ID) == 0:\n",
    "    # Draw the valiation patients IDs at random\n",
    "    validation_patients_ID = np.random.choice(unique_patients_list, num_validation)\n",
    "\n",
    "    \n",
    "\n",
    "print(\"\\nvaliation Patients IDs:\")\n",
    "print(validation_patients_ID)\n",
    "\n",
    "# Create the partition entry\n",
    "DICOM_dataframe[\"Partition\"] = \"\"\n",
    "DICOM_dataframe[\"Partition\"] = \"Train\"\n",
    "\n",
    "# Replace the valiation patients with the valiation flag\n",
    "DICOM_dataframe.loc[DICOM_dataframe['PatientID'].isin(validation_patients_ID), \"Partition\"] = \"Validation\"\n",
    "\n",
    "# Print info\n",
    "num_sample_train = np.sum(DICOM_dataframe.Partition == \"Train\")\n",
    "num_sample_valiation = np.sum(DICOM_dataframe.Partition == \"Validation\")\n",
    "print(\"\\n%d train patients. %d samples.\"%(num_uniques-num_validation, num_sample_train))\n",
    "print(\"%d validation patients. %d samples.\"%(num_validation, num_sample_valiation))\n",
    "print(\"\\nvalidation/train fraction = %f.\"%(num_sample_valiation/num_sample_train))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The couch removal was not perfect, some samples must be fine tunned:\n",
    "couch_removal_skip = list()\n",
    "list_conf_1 = ['HNSCC-01-0027','HNSCC-01-0107']\n",
    "list_conf_2 = ['HNSCC-01-0121','HNSCC-01-0095','HNSCC-01-0107','HNSCC-01-0081','HNSCC-01-0118','HNSCC-01-0212','HNSCC-01-0137','HNSCC-01-0091','HNSCC-01-0093']\n",
    "list_conf_3 = ['HNSCC-01-0091']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all sub-sampled datasets\n",
    "write_train_list = list()\n",
    "write_validation_list = list()\n",
    "\n",
    "\n",
    "OUTPUT_NAME_TRAIN = os.path.join(OUTPUT_PATH, 'Train_Dataset')\n",
    "OUTPUT_NAME_VALIDATION = os.path.join(OUTPUT_PATH, 'Validation_Dataset')\n",
    "\n",
    "for idx_downSample in range(DOWNSAMPLE_SCALES):\n",
    "\n",
    "    scale_factor = 2**idx_downSample\n",
    "\n",
    "    NAME_APPEND = \"_%dx%dx%d\"%(Objective_size[0]/scale_factor,\n",
    "                                Objective_size[1]/scale_factor,\n",
    "                                Objective_size[2]/scale_factor)\n",
    "\n",
    "    write_train = tf.io.TFRecordWriter(OUTPUT_NAME_TRAIN+NAME_APPEND+'.tfrecord')\n",
    "    write_validation = tf.io.TFRecordWriter(OUTPUT_NAME_VALIDATION+NAME_APPEND+'.tfrecord')\n",
    "\n",
    "    write_train_list.append(write_train)\n",
    "    write_validation_list.append(write_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "order_zoom_use = 0\n",
    "\n",
    "tic = time.time()\n",
    "\n",
    "col_names =  ['PatientID',\n",
    "              'Location',\n",
    "              'CT_min',\n",
    "              'CT_max',\n",
    "              'CT_5_lim',\n",
    "              'PET_min',\n",
    "              'PET_max',\n",
    "              'PET_5_lim',\n",
    "              'PET_bone_mean',\n",
    "              'PET_parenchyma_mean',\n",
    "              'PET_fluids-fat_mean',\n",
    "              'PET_air_mean',\n",
    "              'X_size',\n",
    "              'Y_size',\n",
    "              'Z_size']\n",
    "\n",
    "\n",
    "DATASET_frame = pd.DataFrame(columns=col_names)\n",
    "\n",
    "# Limits margin\n",
    "HU_Lung_upper_limit = HU_Lung_upper_limit\n",
    "HU_Bone_upper_limit = HU_Bone_upper_limit\n",
    "HU_Fluids_Fat_lower_limit = HU_Fluids_Fat_lower_limit * 1.2\n",
    "\n",
    "# Used to clean the image histograms\n",
    "PORC_HIST_LIM_PET = 0.01\n",
    "PORC_HIST_LIM_CT = 0.01\n",
    "\n",
    "# CT dynamic range limits\n",
    "LIM_SUP_CT = (((HU_Bone_upper_limit/1000.0)+1)*DH.mu_agua_120kev/10.0)\n",
    "LIM_INF_CT = (((HU_Fluids_Fat_lower_limit/1000.0)+1)*DH.mu_agua_120kev/10.0) \n",
    "\n",
    "# Histogram analysis\n",
    "Lim_bines = 3\n",
    "Lim_bines_test = 35\n",
    "LIM_SUP_CT\n",
    "\n",
    "# For each patient\n",
    "patient_list = DICOM_dataframe.PatientID.unique()\n",
    "idx_pat = 0\n",
    "for patient in patient_list:\n",
    "\n",
    "    idx_pat = idx_pat + 1\n",
    "    print('(%d/%d) Loading patient: %s'%(idx_pat,len(patient_list), patient))\n",
    "\n",
    "\n",
    "    # and each study\n",
    "    patient_df = DICOM_dataframe.loc[DICOM_dataframe['PatientID'] == patient]\n",
    "    # get all studies of this patient\n",
    "    study_list = patient_df.Study.unique()\n",
    "\n",
    "    study_number = 0\n",
    "    for study_id in study_list:\n",
    "        # And create one entry...\n",
    "\n",
    "        tac = time.time()\n",
    "\n",
    "        study_df = patient_df.loc[patient_df['Study'] == study_id]\n",
    "\n",
    "        ct_path = HNSCC_dataframe.Location.loc[HNSCC_dataframe.index[(HNSCC_dataframe.PatientID == patient) & (HNSCC_dataframe.Study == study_id) & (HNSCC_dataframe.Type == 'CT')  ]].values[0]\n",
    "        pet_path = HNSCC_dataframe.Location.loc[HNSCC_dataframe.index[(HNSCC_dataframe.PatientID == patient) & (HNSCC_dataframe.Study == study_id) & (HNSCC_dataframe.Type == 'PET_NO_AC')  ]].values[0]\n",
    "\n",
    "        # Get all slice paths\n",
    "        list_paths_ct = sorted(os.listdir(ct_path))\n",
    "        list_paths_pet = sorted(os.listdir(pet_path))\n",
    "\n",
    "        # Allocate memory for input\n",
    "        ct_input = np.zeros((study_df.CT_Vol_Shape_X.values[0],\n",
    "                             study_df.CT_Vol_Shape_Y.values[0],\n",
    "                             study_df.CT_Vol_Shape_Z.values[0]), dtype=np.float32)\n",
    "        pet_input = np.zeros((study_df.PET_Vol_Shape_X.values[0],\n",
    "                              study_df.PET_Vol_Shape_Y.values[0],\n",
    "                              study_df.PET_Vol_Shape_Z.values[0]), dtype=np.float32)\n",
    "\n",
    "        # Load volumes\n",
    "        locations_CT = np.zeros((study_df.CT_Vol_Shape_Z.values[0]))\n",
    "        for idx, filename in enumerate(list_paths_ct):\n",
    "            if \".dcm\" in filename.lower():  # check whether the file's DICOM\n",
    "                RefDs = dicom.read_file(os.path.join(ct_path,filename))\n",
    "                ct_input[:,:,idx] = (RefDs.pixel_array*RefDs.RescaleSlope)+RefDs.RescaleIntercept\n",
    "                locations_CT[idx] = RefDs.SliceLocation\n",
    "\n",
    "        # Reorder data to its actual position in space\n",
    "        orden_fix = np.argsort(locations_CT)\n",
    "        ct_input = ct_input[:,:,orden_fix]\n",
    "\n",
    "        # Remove coach from CT\n",
    "        if patient in couch_removal_skip or (patient == 'HNSCC-01-0091' and study_number == 1):\n",
    "            print_ct_orig = np.copy(ct_input[:,:,int(ct_input.shape[2]/2)])\n",
    "            print_ct_final = np.copy(ct_input[:,:,int(ct_input.shape[2]/2)])\n",
    "            couch_mask = np.zeros(print_ct_orig.shape)\n",
    "            print('No couch removal possible.')\n",
    "        else:\n",
    "            B_n_d = 4\n",
    "            B_n_e = 2\n",
    "            B_d_d = 15\n",
    "            B_d_e = 13\n",
    "            limits_ct_use = [0,0]\n",
    "            corr_disk = 6\n",
    "            if patient == list_conf_1:\n",
    "                B_n_d = 0\n",
    "                B_n_e = 0\n",
    "            if patient in list_conf_2:\n",
    "                B_n_d = 2\n",
    "                B_n_e = 2\n",
    "            if patient in list_conf_3:\n",
    "                limits_ct_use[1] = int(ct_input.shape[2]/2)\n",
    "            if patient == 'HNSCC-01-0114' and study_number == 1:\n",
    "                corr_disk = 3\n",
    "\n",
    "\n",
    "            couch_mask = DH.CT_couch_removal_mask_multicore(ct_input, \n",
    "                                                               samples_use = 20, \n",
    "                                                               n_corr_disk = corr_disk, \n",
    "                                                               limits_CT = limits_ct_use,\n",
    "                                                               B_n_dilatation_disk_rad = B_n_d,\n",
    "                                                               B_n_erosion_disk_rad = B_n_e,\n",
    "                                                               B_d_dilatation_disk_rad = B_d_d,\n",
    "                                                               B_d_erosion_disk_rad = B_d_e) \n",
    "            print_ct_orig = np.copy(ct_input[:,:,int(ct_input.shape[2]/2)])\n",
    "            for z_idx in range(0,ct_input.shape[2]):\n",
    "                ct_input[:,:,z_idx][couch_mask] = HU_Air_limit\n",
    "            print_ct_final = np.copy(ct_input[:,:,int(ct_input.shape[2]/2)])\n",
    "\n",
    "        # Convert from HU tu linear attenuation coeficient\n",
    "        ct_input = ((ct_input/1000.0)+1)*DH.mu_agua_120kev/10.0\n",
    "        ct_input[ct_input<0]=0\n",
    "\n",
    "        locations_PET = np.zeros((study_df.PET_Vol_Shape_Z.values[0]))\n",
    "        time_reference = np.zeros((study_df.PET_Vol_Shape_Z.values[0]))\n",
    "        for idx, filename in enumerate(list_paths_pet):\n",
    "            if \".dcm\" in filename.lower():  # check whether the file's DICOM\n",
    "                RefDs = dicom.read_file(os.path.join(pet_path,filename))\n",
    "                if RefDs.Units == 'PROPCNTS':\n",
    "                    pet_input[:,:,idx] = ((RefDs.pixel_array*RefDs.RescaleSlope)+RefDs.RescaleIntercept) / (RefDs.ActualFrameDuration/1000.0)\n",
    "                else:\n",
    "                    pet_input[:,:,idx] = ((RefDs.pixel_array*RefDs.RescaleSlope)+RefDs.RescaleIntercept) #*RefDs.DecayFactor\n",
    "                locations_PET[idx] = RefDs.SliceLocation\n",
    "                time_reference[idx] = RefDs.FrameReferenceTime\n",
    "\n",
    "        # PET data to SUVlbm\n",
    "        if RefDs.PatientSex == 'M':\n",
    "            # SUVlbm(Janma): males: 9.27E3 * weight / (6.68E3 + 216 * weight / (height^2))\n",
    "            pet_input = pet_input/(9.27e3 * RefDs.PatientWeight / (6.68e3 + 216 * RefDs.PatientWeight / (RefDs.PatientSize**2)))\n",
    "        else:\n",
    "            # SUVlbm(Janma): females: 9.27E3 * weight / (8.78E3 + 244 * weight / (height^2))\n",
    "            pet_input = pet_input/(9.27e3 * RefDs.PatientWeight / (8.75e3 + 244 * RefDs.PatientWeight / (RefDs.PatientSize**2)))\n",
    "        \n",
    "        # Reorder data to its actual position in space\n",
    "        orden_fix = np.argsort(locations_PET)\n",
    "        pet_input = pet_input[:,:,orden_fix]\n",
    "\n",
    "        pet_input[pet_input<0]=0\n",
    "        \n",
    "        \n",
    "        # Get voxels size\n",
    "        locations_PET = np.sort(locations_PET)\n",
    "        locations_CT = np.sort(locations_CT)\n",
    "        Voxel_size_INPUT = np.zeros((2,3), dtype=np.float32)\n",
    "        Voxel_size_INPUT[0,0] = study_df.CT_Voxel_size_X.values[0]\n",
    "        Voxel_size_INPUT[0,1] = study_df.CT_Voxel_size_Y.values[0]\n",
    "        Voxel_size_INPUT[0,2] = (locations_CT[1]-locations_CT[0])#study_df.CT_Voxel_size_Z.values[0]\n",
    "        Voxel_size_INPUT[1,0] = study_df.PET_Voxel_size_X.values[0]\n",
    "        Voxel_size_INPUT[1,1] = study_df.PET_Voxel_size_Y.values[0]\n",
    "        Voxel_size_INPUT[1,2] = (locations_PET[1]-locations_PET[0])#study_df.PET_Voxel_size_Z.values[0]\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        # Normalize volume size\n",
    "        ct_output, pet_output, limits_Z = DH.normalize_volume_size(ct_input, \n",
    "                                                                      pet_input, \n",
    "                                                                      locations_CT, \n",
    "                                                                      locations_PET, \n",
    "                                                                      Voxel_size_INPUT, \n",
    "                                                                      Voxel_size,\n",
    "                                                                      Objective_size)\n",
    "\n",
    "\n",
    "        # Calculate dataframe info\n",
    "\n",
    "        # Get CT histogram at tiesue data location           \n",
    "        bines_ct = np.linspace(LIM_INF_CT,LIM_SUP_CT*2,128)\n",
    "        vals_hst_ct, _ = np.histogram(ct_output.flatten(), bins=bines_ct)\n",
    "        ct_low_lim = bines_ct[np.argwhere(vals_hst_ct[Lim_bines:] < (PORC_HIST_LIM_CT*vals_hst_ct[Lim_bines:].max()))[0]][0]\n",
    "        bines_ct = np.linspace(LIM_INF_CT,LIM_SUP_CT,128)\n",
    "        vals_hst_ct, _ = np.histogram(ct_output.flatten(), bins=bines_ct)\n",
    "\n",
    "        # Get PET histogram\n",
    "        # Analize only those voxels where there is body mass\n",
    "        # First get a binary mask of the body\n",
    "        binary_mask = np.zeros(ct_output.shape)\n",
    "        binary_mask[ct_output > LIM_INF_CT] = 1\n",
    "        # Dilate binary mask\n",
    "        binary_mask = ndimage.morphology.binary_dilation(binary_mask,structure=morphology.ball(3))\n",
    "        # Get voxels to be analized\n",
    "        pet_vector_hist = pet_output[binary_mask > 0].flatten()\n",
    "        # get histogram\n",
    "        bines_pet = np.linspace(0,pet_vector_hist.max(),4096*8)\n",
    "        vals_hst_pet, _ = np.histogram(pet_vector_hist, bins=bines_pet)\n",
    "        # Look for most repeated value\n",
    "        bin_max_rep_pet = vals_hst_pet[Lim_bines_test:].max()\n",
    "        # Select a upper limit abobe this point\n",
    "        pet_thrs_lim = bines_pet[Lim_bines+np.argmax(vals_hst_pet[Lim_bines:])+np.argwhere(vals_hst_pet[Lim_bines+np.argmax(vals_hst_pet[Lim_bines:]):] < bin_max_rep_pet*PORC_HIST_LIM_PET)[0]][0]\n",
    "        # Create a new histogram\n",
    "        bines_pet = np.linspace(bines_pet[Lim_bines],pet_thrs_lim,128, endpoint=False)\n",
    "        vals_hst_pet, _ = np.histogram(pet_vector_hist, bins=bines_pet)\n",
    "        # Generate datapoint\n",
    "        pet_low_lim = pet_thrs_lim\n",
    "\n",
    "        # Generate input and labels\n",
    "        (input_net, labels_out) = DH.preprocess_sample(ct_output, \n",
    "                                                          pet_output, \n",
    "                                                          pet_low_lim, \n",
    "                                                          LIM_INF_CT, \n",
    "                                                          HU_Lung_upper_limit, \n",
    "                                                          HU_Parenchyma_lower_limit,\n",
    "                                                          HU_Parenchyma_upper_limit, \n",
    "                                                          HU_Bone_low_limit, \n",
    "                                                          HU_Bone_upper_limit,\n",
    "                                                          HU_Fluids_Fat_lower_limit,\n",
    "                                                          HU_Fluids_Fat_upper_limit,\n",
    "                                                          USE_FAT_THRESHOLD = True,\n",
    "                                                          USE_HIST_THRESHOLD = False)\n",
    "\n",
    "        # Clean labels from interpolation errors\n",
    "        labels_out[labels_out <= 0.3] = 0\n",
    "        labels_out[labels_out > 0.3] = 1\n",
    "        # Remove double labeling of pixels whith priority order Bones -> parenchyma -> fuid-fat -> Air\n",
    "        labels_out[:,:,:,2][(labels_out[:,:,:,0]+labels_out[:,:,:,1]) > 0] = 0\n",
    "        labels_out[:,:,:,1][labels_out[:,:,:,0] > 0] = 0\n",
    "        labels_out[:,:,:,3][labels_out[:,:,:,0] > 0] = 0\n",
    "        labels_out[:,:,:,3][labels_out[:,:,:,1] > 0] = 0\n",
    "        labels_out[:,:,:,3][labels_out[:,:,:,2] > 0] = 0\n",
    "\n",
    "        # Get mean bone, parenchyma, fluid-fat and air mean PET activity\n",
    "        PET_bone_mean = np.mean(pet_output[labels_out[:,:,:,0]==1])\n",
    "        PET_parenchyma_mean = np.mean(pet_output[labels_out[:,:,:,1]==1])\n",
    "        PET_fluid_fat_mean = np.mean(pet_output[labels_out[:,:,:,2]==1])\n",
    "        PET_air_mean = np.mean(pet_output[labels_out[:,:,:,3]==1])\n",
    "\n",
    "        # Clamp PET at double of bone activity\n",
    "        clamp_mult = 10.0\n",
    "        input_net[input_net > (clamp_mult*PET_bone_mean)]  = clamp_mult*PET_bone_mean\n",
    "\n",
    "        # Clamp CT volume\n",
    "        ct_out_scale = np.copy(ct_output)\n",
    "        ct_out_scale[ct_out_scale < LIM_INF_CT]  = LIM_INF_CT\n",
    "        ct_out_scale[ct_out_scale > LIM_SUP_CT]  = LIM_SUP_CT\n",
    "\n",
    "        # Normalize to range 0:1 \n",
    "        input_net = (input_net - input_net.min()) / (input_net.max() - input_net.min())\n",
    "        ct_out_scale = (ct_out_scale - ct_out_scale.min()) / (ct_out_scale.max() - ct_out_scale.min())\n",
    "\n",
    "        # Create the Example\n",
    "        example = tf.train.Example(\n",
    "            features=tf.train.Features(feature={'input': tf.train.Feature(float_list=tf.train.FloatList(value= np.reshape(input_net, np.prod(input_net.shape)) )),\n",
    "                                                'label': tf.train.Feature(float_list=tf.train.FloatList(value= np.reshape(labels_out, np.prod(labels_out.shape)) )),\n",
    "                                                'ct': tf.train.Feature(float_list=tf.train.FloatList(value= np.reshape(ct_out_scale, np.prod(ct_out_scale.shape)) )),\n",
    "                                                'limits_Z': tf.train.Feature(float_list=tf.train.FloatList(value= np.reshape(limits_Z, np.prod(limits_Z.shape)) ))\n",
    "                                                }))\n",
    "\n",
    "\n",
    "        if study_df.Partition.values[0] == \"Train\":\n",
    "            write_train_list[0].write(example.SerializeToString())\n",
    "            write_train_list[0].flush()\n",
    "            OUTPUT_NAME = OUTPUT_NAME_TRAIN\n",
    "        elif study_df.Partition.values[0] == \"Validation\":\n",
    "            write_validation_list[0].write(example.SerializeToString())\n",
    "            write_validation_list[0].flush()\n",
    "            OUTPUT_NAME = OUTPUT_NAME_VALIDATION\n",
    "        else:\n",
    "            raise ValueError('Study not in Train nor Validation list!')\n",
    "\n",
    "\n",
    "\n",
    "        # Create low resolution Example\n",
    "        for idx_downSample in range(1, DOWNSAMPLE_SCALES):\n",
    "            low_res_post_scaling = 2**idx_downSample\n",
    "\n",
    "            input_net_low_res = ndimage.interpolation.zoom(input=input_net[:,:,:], zoom=[1.0/low_res_post_scaling,1.0/low_res_post_scaling,1.0/low_res_post_scaling], order=order_zoom_use)\n",
    "            ct_out_scale_low_res = ndimage.interpolation.zoom(input=ct_out_scale[:,:,:], zoom=[1.0/low_res_post_scaling,1.0/low_res_post_scaling,1.0/low_res_post_scaling], order=order_zoom_use)\n",
    "            labels_out_low_res = np.zeros((int(labels_out.shape[0]/low_res_post_scaling),int(labels_out.shape[1]/low_res_post_scaling),int(labels_out.shape[2]/low_res_post_scaling),labels_out.shape[-1]))            \n",
    "            for idx_label in range(labels_out.shape[-1]):\n",
    "                labels_out_low_res[:,:,:,idx_label] = ndimage.interpolation.zoom(input=labels_out[:,:,:,idx_label], zoom=[1.0/low_res_post_scaling,1.0/low_res_post_scaling,1.0/low_res_post_scaling], order=order_zoom_use)\n",
    "            limits_Z_low_res = np.array([int(limits_Z[0]/low_res_post_scaling),int(limits_Z[1]/low_res_post_scaling)])\n",
    "\n",
    "            # Clean low resolution labels from interpolation errors\n",
    "            labels_out_low_res[labels_out_low_res<0.5] = 0\n",
    "            labels_out_low_res[labels_out_low_res>=0.5] = 1\n",
    "            # Remove double labeling of pixels whith priority order Bones -> parenchyma -> fuid-fat -> Air\n",
    "            labels_out[:,:,:,2][(labels_out[:,:,:,0]+labels_out[:,:,:,1]) > 0] = 0\n",
    "            labels_out[:,:,:,1][labels_out[:,:,:,0] > 0] = 0\n",
    "            labels_out[:,:,:,3][labels_out[:,:,:,0] > 0] = 0\n",
    "            labels_out[:,:,:,3][labels_out[:,:,:,1] > 0] = 0\n",
    "            labels_out[:,:,:,3][labels_out[:,:,:,2] > 0] = 0\n",
    "\n",
    "            example_low_res = tf.train.Example(\n",
    "                features=tf.train.Features(feature={'input': tf.train.Feature(float_list=tf.train.FloatList(value= np.reshape(input_net_low_res, np.prod(input_net_low_res.shape)) )),\n",
    "                                                    'label': tf.train.Feature(float_list=tf.train.FloatList(value= np.reshape(labels_out_low_res, np.prod(labels_out_low_res.shape)) )),\n",
    "                                                    'ct': tf.train.Feature(float_list=tf.train.FloatList(value= np.reshape(ct_out_scale_low_res, np.prod(ct_out_scale_low_res.shape)) )),\n",
    "                                                    'limits_Z': tf.train.Feature(float_list=tf.train.FloatList(value= np.reshape(limits_Z_low_res, np.prod(limits_Z_low_res.shape)) ))\n",
    "                                                    }))\n",
    "\n",
    "\n",
    "\n",
    "            if study_df.Partition.values[0] == \"Train\":\n",
    "                write_train_list[idx_downSample].write(example_low_res.SerializeToString())\n",
    "                write_train_list[idx_downSample].flush()\n",
    "            elif study_df.Partition.values[0] == \"Validation\":\n",
    "                write_validation_list[idx_downSample].write(example_low_res.SerializeToString())\n",
    "                write_validation_list[idx_downSample].flush()\n",
    "            else:\n",
    "                raise ValueError('Study not in Train nor Validation list!')\n",
    "                exit()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Add to dataframe\n",
    "        DATASET_frame.loc[len(DATASET_frame)] = [patient, \n",
    "                                                 OUTPUT_NAME,\n",
    "                                                 ct_output.min(),\n",
    "                                                 ct_output.max(),\n",
    "                                                 ct_low_lim,\n",
    "                                                 pet_output.min(),\n",
    "                                                 pet_output.max(),\n",
    "                                                 pet_low_lim,\n",
    "                                                 PET_bone_mean,\n",
    "                                                 PET_parenchyma_mean,\n",
    "                                                 PET_fluid_fat_mean,\n",
    "                                                 PET_air_mean,\n",
    "                                                Objective_size[0],\n",
    "                                                Objective_size[1],\n",
    "                                                Objective_size[2]]        \n",
    "\n",
    "        print(\"\\tLimit: \\n\\t\\tCT:%0.5f (%d %%)\\n\\t\\tPET:%e (%d %%)\"%(ct_low_lim,PORC_HIST_LIM_CT*100,\n",
    "                                                                        pet_low_lim,PORC_HIST_LIM_PET*100,))\n",
    "\n",
    "\n",
    "\n",
    "        # Print all figures            \n",
    "        plt.figure(dpi = 50)\n",
    "        ax = plt.subplot(1, 2, 1)\n",
    "        ax.stem(bines_ct[Lim_bines:-1], vals_hst_ct[Lim_bines:])\n",
    "        ax = plt.subplot(1, 2, 2)\n",
    "        ax.stem(bines_pet[Lim_bines:-1], vals_hst_pet[Lim_bines:])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "        print('Max-Min input = %f ; %f'%(input_net.max(),input_net.min()))\n",
    "        print('Max-Min ct = %f ; %f'%(ct_out_scale.max(),ct_out_scale.min()))\n",
    "        print('Limits Min-Max Z = %f ; %f'%(limits_Z[0],limits_Z[1]))\n",
    "        print('Pet means = %e ; %e ; %e ; %e'%(PET_bone_mean,PET_parenchyma_mean,PET_fluid_fat_mean,PET_air_mean))\n",
    "\n",
    "        X_ct = int(ct_output.shape[0]/2)\n",
    "        Y_ct = int(ct_output.shape[1]/2)\n",
    "        Z_ct = int(ct_output.shape[2]/2)\n",
    "\n",
    "        X_pet = int(pet_output.shape[0]/2)\n",
    "        Y_pet = int(pet_output.shape[1]/2)\n",
    "        Z_pet = int(pet_output.shape[2]/2)\n",
    "\n",
    "        print(\"\\tCT couch removal test:\")\n",
    "        plt.figure(dpi=100)\n",
    "        ax = plt.subplot(2,3,1)\n",
    "        ax.imshow(print_ct_orig)\n",
    "        ax = plt.subplot(2,3,2)\n",
    "        ax.imshow(couch_mask)\n",
    "        ax = plt.subplot(2,3,3)\n",
    "        ax.imshow(print_ct_final)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\tOriginal volumes:\")\n",
    "        plt.figure(dpi=100)\n",
    "        ax = plt.subplot(3,2,1)\n",
    "        ax.imshow(ct_output[:,:,Z_ct])\n",
    "        ax = plt.subplot(3,2,2)\n",
    "        ax.imshow(pet_output[:,:,Z_pet])\n",
    "        ax = plt.subplot(3,2,3)\n",
    "        ax.imshow(ct_output[:,Y_ct,:])\n",
    "        ax = plt.subplot(3,2,4)\n",
    "        ax.imshow(pet_output[:,Y_pet,:])\n",
    "        ax = plt.subplot(3,2,5)\n",
    "        ax.imshow(ct_output[X_ct,:,:])\n",
    "        ax = plt.subplot(3,2,6)\n",
    "        ax.imshow(pet_output[X_pet,:,:])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        X_ct = int(input_net.shape[0]/2)\n",
    "        Y_ct = int(input_net.shape[1]/2)\n",
    "        Z_ct = int(input_net.shape[2]/2)\n",
    "\n",
    "        X_pet = int(input_net.shape[0]/2)\n",
    "        Y_pet = int(input_net.shape[1]/2)\n",
    "        Z_pet = int(input_net.shape[2]/2)\n",
    "\n",
    "\n",
    "        print(\"\\t Network Input and CT Volume:\")\n",
    "        plt.figure(dpi=100)\n",
    "        ax = plt.subplot(3,2,1)\n",
    "        ax.imshow(input_net[:,:,Z_pet])\n",
    "        ax = plt.subplot(3,2,2)\n",
    "        ax.imshow(ct_out_scale[:,:,Z_ct])\n",
    "        ax = plt.subplot(3,2,3)\n",
    "        ax.imshow(input_net[:,Y_pet,:])\n",
    "        ax = plt.subplot(3,2,4)\n",
    "        ax.imshow(ct_out_scale[:,X_ct,:])\n",
    "        ax = plt.subplot(3,2,5)\n",
    "        ax.imshow(input_net[X_pet,:,:])\n",
    "        ax = plt.subplot(3,2,6)\n",
    "        ax.imshow(ct_out_scale[X_ct,:,:])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "        print(\"\\tLabels Bone and Parenchyma (except lung):\")\n",
    "        plt.figure(dpi=100)\n",
    "        ax = plt.subplot(3,2,1)\n",
    "        ax.imshow(labels_out[:,:,Z_ct,0])\n",
    "        ax = plt.subplot(3,2,2)\n",
    "        ax.imshow(labels_out[:,:,Z_pet,1])\n",
    "        ax = plt.subplot(3,2,3)\n",
    "        ax.imshow(labels_out[:,Y_ct,:,0])\n",
    "        ax = plt.subplot(3,2,4)\n",
    "        ax.imshow(labels_out[:,Y_pet,:,1])\n",
    "        ax = plt.subplot(3,2,5)\n",
    "        ax.imshow(labels_out[X_ct,:,:,0])\n",
    "        ax = plt.subplot(3,2,6)\n",
    "        ax.imshow(labels_out[X_pet,:,:,1])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        print(\"\\tLabel Fluid-Fat and Air:\")\n",
    "        plt.figure(dpi=100)\n",
    "        ax = plt.subplot(3,2,1)\n",
    "        ax.imshow(labels_out[:,:,Z_pet,2])\n",
    "        ax = plt.subplot(3,2,2)\n",
    "        ax.imshow(labels_out[:,:,Z_pet,3])\n",
    "        ax = plt.subplot(3,2,3)\n",
    "        ax.imshow(labels_out[:,Y_pet,:,2])\n",
    "        ax = plt.subplot(3,2,4)\n",
    "        ax.imshow(labels_out[:,Y_pet,:,3])\n",
    "        ax = plt.subplot(3,2,5)\n",
    "        ax.imshow(labels_out[X_pet,:,:,2])\n",
    "        ax = plt.subplot(3,2,6)\n",
    "        ax.imshow(labels_out[X_pet,:,:,3])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        del(ct_output)\n",
    "        del(pet_output)\n",
    "\n",
    "        print('\\tProcess time: %f'%(time.time()-tac))\n",
    "        study_number = study_number + 1\n",
    "\n",
    "\n",
    "elap_min = ((time.time()-tic)/60.0)\n",
    "print('Total elapsed time: %d hs %d min'%((elap_min/60.0),(((elap_min/60.0)-(elap_min//60.0))*60.0)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
